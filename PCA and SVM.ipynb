{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VDVmc6kkoRl",
        "outputId": "542224d6-4ec3-4709-82bb-65f31df88319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (891, 6)\n",
            "Unique labels (original): ['C' 'NaN_label' 'Q' 'S']\n",
            "Number of classes: 4\n",
            "Train shape: (713, 6)\n",
            "Test shape: (178, 6)\n",
            "\n",
            "PCA chose components: 6\n",
            "Explained variance ratio of chosen components:\n",
            "[0.30657111 0.27796526 0.16027894 0.10473784 0.0894279  0.06101895]\n",
            "\n",
            "==========================\n",
            "WITHOUT PCA\n",
            "=========================\n",
            "Logistic Regression accuracy: 0.6685393258426966\n",
            "Linear SVM accuracy: 0.6629213483146067\n",
            "\n",
            "==========================\n",
            "WITH PCA\n",
            "==========================\n",
            "Logistic Regression (PCA) accuracy: 0.6685393258426966\n",
            "Linear SVM (PCA) accuracy: 0.6629213483146067\n",
            "\n",
            "=========== SUMMARY ==========\n",
            "Logistic Regression  (no PCA): 0.6685\n",
            "Logistic Regression  (PCA)   : 0.6685\n",
            "SVM (no PCA)                 : 0.6629\n",
            "SVM (PCA)                    : 0.6629\n",
            "Original classes mapping: {0: 'C', 1: 'NaN_label', 2: 'Q', 3: 'S'}\n",
            "===============================\n"
          ]
        }
      ],
      "source": [
        "# Research AI Task 9\n",
        "\n",
        "# - Perform PCA on the given dataset\n",
        "# - Implement a Support Vector Machine (SVM) from scratch\n",
        "# - Implement Logistic Regression from scratch\n",
        "# - Compare SVM and Logistic Regression, with and without PCA\n",
        "\n",
        "# ASSUMPTIONS\n",
        "# -----------\n",
        "# - The dataset is a CSV file.\n",
        "# - All columns except the LAST one are numeric features.\n",
        "# - The LAST column is the target label (can be string or numeric).\n",
        "# - Works for binary or multi-class classification (one-vs-rest).\n",
        "\n",
        "# HOW TO RUN\n",
        "# ----------\n",
        "# python task9_pca_svm_lr.py\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ==============================\n",
        "# 1. Load Dataset\n",
        "# ==============================\n",
        "\n",
        "# Change this to your CSV file name if needed\n",
        "DATA_PATH = \"/content/titanic (3).csv\"\n",
        "\n",
        "data = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Labels: last column\n",
        "y_raw = data.iloc[:, -1].values\n",
        "\n",
        "# Features: all columns except the last one\n",
        "# Identify and drop non-numeric columns that cannot be converted to float\n",
        "# Also drop 'PassengerId' as it's an identifier\n",
        "features_df = data.iloc[:, :-1]\n",
        "\n",
        "# List of columns to drop (non-numeric or identifiers, common in Titanic dataset)\n",
        "columns_to_drop_from_features = ['PassengerId', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\n",
        "\n",
        "# Drop columns. 'errors='ignore'' will prevent error if a column is not found\n",
        "X = features_df.drop(columns=columns_to_drop_from_features, errors='ignore').values.astype(float)\n",
        "\n",
        "print(\"Dataset shape:\", X.shape)\n",
        "\n",
        "# Ensure y_raw is consistently string type, handling NaNs\n",
        "y_raw = pd.Series(y_raw).fillna('NaN_label').astype(str).values\n",
        "print(\"Unique labels (original):\", np.unique(y_raw))\n",
        "\n",
        "# Encode labels to integers 0..K-1\n",
        "classes, y_int = np.unique(y_raw, return_inverse=True)\n",
        "y = y_int               # 0..K-1\n",
        "num_classes = len(classes)\n",
        "\n",
        "print(\"Number of classes:\", num_classes)\n",
        "\n",
        "# ==============================\n",
        "# Imputation (from scratch) - Added to handle NaNs\n",
        "# ==============================\n",
        "class SimpleImputer:\n",
        "    def __init__(self, strategy='mean'):\n",
        "        self.strategy = strategy\n",
        "        self.fill_value = None\n",
        "\n",
        "    def fit(self, X):\n",
        "        if self.strategy == 'mean':\n",
        "            self.fill_value = np.nanmean(X, axis=0)\n",
        "        elif self.strategy == 'median':\n",
        "            self.fill_value = np.nanmedian(X, axis=0)\n",
        "        elif self.strategy == 'constant':\n",
        "            # You can specify a constant value if needed, e.g., self.constant_value\n",
        "            self.fill_value = np.zeros(X.shape[1]) # Fill with zeros as a default constant\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_imputed = X.copy()\n",
        "        # Handle cases where entire column might be NaN after split\n",
        "        # Use a small constant if fill_value itself is NaN for a column\n",
        "        for i in range(X.shape[1]):\n",
        "            if np.isnan(self.fill_value[i]):\n",
        "                X_imputed[np.isnan(X_imputed[:, i]), i] = 0.0 # Fallback to 0 if mean is NaN\n",
        "            else:\n",
        "                X_imputed[np.isnan(X_imputed[:, i]), i] = self.fill_value[i]\n",
        "        return X_imputed\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        self.fit(X)\n",
        "        return self.transform(X)\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X = imputer.fit_transform(X)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 2. Train / Test Split\n",
        "# ==============================\n",
        "\n",
        "def train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42):\n",
        "    n_samples = X.shape[0]\n",
        "    indices = np.arange(n_samples)\n",
        "    if shuffle:\n",
        "        rng = np.random.RandomState(random_state)\n",
        "        rng.shuffle(indices)\n",
        "\n",
        "    test_size_int = int(n_samples * test_size)\n",
        "    test_idx = indices[:test_size_int]\n",
        "    train_idx = indices[test_size_int:]\n",
        "\n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = y[train_idx], y[test_idx]\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)\n",
        "\n",
        "# ==============================\n",
        "# 3. Standardization\n",
        "# ==============================\n",
        "\n",
        "class StandardScaler:\n",
        "    def fit(self, X):\n",
        "        self.mean_ = X.mean(axis=0)\n",
        "        self.std_ = X.std(axis=0)\n",
        "        self.std_[self.std_ == 0] = 1.0\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return (X - self.mean_) / self.std_\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        self.fit(X)\n",
        "        return self.transform(X)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ==============================\n",
        "# 4. PCA (from scratch)\n",
        "# ==============================\n",
        "\n",
        "class PCA:\n",
        "    def __init__(self, n_components=None, var_threshold=None):\n",
        "        \"\"\"\n",
        "        n_components: fixed number of components to keep\n",
        "        var_threshold: if given, choose smallest number of components\n",
        "                       explaining at least this fraction of variance.\n",
        "        \"\"\"\n",
        "        self.n_components = n_components\n",
        "        self.var_threshold = var_threshold\n",
        "\n",
        "    def fit(self, X):\n",
        "        # Covariance matrix of standardized data\n",
        "        cov_matrix = np.cov(X.T)  # (d, d)\n",
        "\n",
        "        # Eigen decomposition of symmetric matrix\n",
        "        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
        "\n",
        "        # Sort eigenvalues/vectors in descending order\n",
        "        idxs = np.argsort(eigenvalues)[::-1]\n",
        "        eigenvalues = eigenvalues[idxs]\n",
        "        eigenvectors = eigenvectors[:, idxs]\n",
        "\n",
        "        explained_variance_ratio = eigenvalues / np.sum(eigenvalues)\n",
        "\n",
        "        # Decide number of components\n",
        "        if self.var_threshold is not None and self.n_components is None:\n",
        "            cumulative = np.cumsum(explained_variance_ratio)\n",
        "            self.n_components = np.searchsorted(cumulative, self.var_threshold) + 1\n",
        "\n",
        "        if self.n_components is None:\n",
        "            self.n_components = X.shape[1]\n",
        "\n",
        "        self.components_ = eigenvectors[:, : self.n_components].T  # (k, d)\n",
        "        self.explained_variance_ratio_ = explained_variance_ratio[: self.n_components]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.dot(X, self.components_.T)\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        self.fit(X)\n",
        "        return self.transform(X)\n",
        "\n",
        "# Keep enough components to explain ~95% variance\n",
        "pca = PCA(var_threshold=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(\"\\nPCA chose components:\", pca.n_components)\n",
        "print(\"Explained variance ratio of chosen components:\")\n",
        "print(pca.explained_variance_ratio_)\n",
        "\n",
        "# ==============================\n",
        "# 5. Logistic Regression (from scratch)\n",
        "# ==============================\n",
        "\n",
        "class LogisticRegressionBinary:\n",
        "    def __init__(self, lr=0.01, n_iters=1000):\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # y is 0 or 1\n",
        "        n_samples, n_features = X.shape\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0.0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            linear = np.dot(X, self.w) + self.b\n",
        "            y_pred = self._sigmoid(linear)\n",
        "\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            self.w -= self.lr * dw\n",
        "            self.b -= self.lr * db\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        linear = np.dot(X, self.w) + self.b\n",
        "        return self._sigmoid(linear)\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba >= 0.5).astype(int)\n",
        "\n",
        "\n",
        "class OneVsRestLogistic:\n",
        "    \"\"\"\n",
        "    Multi-class logistic regression using one-vs-rest.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, n_iters=1000):\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.classifiers = []\n",
        "        self.classes_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.classifiers = []\n",
        "\n",
        "        for c in self.classes_:\n",
        "            y_binary = (y == c).astype(int)  # 1 for class c, 0 otherwise\n",
        "            clf = LogisticRegressionBinary(lr=self.lr, n_iters=self.n_iters)\n",
        "            clf.fit(X, y_binary)\n",
        "            self.classifiers.append(clf)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # For each classifier, get probability of being its class\n",
        "        all_probs = []\n",
        "        for clf in self.classifiers:\n",
        "            probs = clf.predict_proba(X)\n",
        "            all_probs.append(probs.reshape(-1, 1))\n",
        "        all_probs = np.hstack(all_probs)  # (n_samples, n_classes)\n",
        "        class_indices = np.argmax(all_probs, axis=1)\n",
        "        return self.classes_[class_indices]\n",
        "\n",
        "# ==============================\n",
        "# 6. Linear SVM (from scratch)\n",
        "# ==============================\n",
        "\n",
        "class LinearSVMBinary:\n",
        "    def __init__(self, lr=0.001, lambda_param=0.01, n_iters=1000):\n",
        "        self.lr = lr\n",
        "        self.lambda_param = lambda_param\n",
        "        self.n_iters = n_iters\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        y should be in {-1, +1}\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0.0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                condition = y[idx] * (np.dot(x_i, self.w) + self.b) >= 1\n",
        "                if condition:\n",
        "                    dw = 2 * self.lambda_param * self.w\n",
        "                    db = 0.0\n",
        "                else:\n",
        "                    dw = 2 * self.lambda_param * self.w - y[idx] * x_i\n",
        "                    db = -y[idx]\n",
        "\n",
        "                self.w -= self.lr * dw\n",
        "                self.b -= self.lr * db\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        return np.dot(X, self.w) + self.b\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = self.decision_function(X)\n",
        "        return np.sign(scores)\n",
        "\n",
        "\n",
        "class OneVsRestSVM:\n",
        "    \"\"\"\n",
        "    Multi-class linear SVM using one-vs-rest strategy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.001, lambda_param=0.01, n_iters=1000):\n",
        "        self.lr = lr\n",
        "        self.lambda_param = lambda_param\n",
        "        self.n_iters = n_iters\n",
        "        self.classifiers = []\n",
        "        self.classes_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.classifiers = []\n",
        "\n",
        "        for c in self.classes_:\n",
        "            y_binary = np.where(y == c, 1, -1)\n",
        "            clf = LinearSVMBinary(\n",
        "                lr=self.lr, lambda_param=self.lambda_param, n_iters=self.n_iters\n",
        "            )\n",
        "            clf.fit(X, y_binary)\n",
        "            self.classifiers.append(clf)\n",
        "\n",
        "    def predict(self, X):\n",
        "        all_scores = []\n",
        "        for clf in self.classifiers:\n",
        "            scores = clf.decision_function(X)\n",
        "            all_scores.append(scores.reshape(-1, 1))\n",
        "        all_scores = np.hstack(all_scores)  # (n_samples, n_classes)\n",
        "        class_indices = np.argmax(all_scores, axis=1)\n",
        "        return self.classes_[class_indices]\n",
        "\n",
        "# ==============================\n",
        "# 7. Accuracy Metric\n",
        "# ==============================\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n",
        "# ==============================\n",
        "# 8. Training and Comparison\n",
        "# ==============================\n",
        "\n",
        "print(\"\\n==========================\")\n",
        "print(\"WITHOUT PCA\")\n",
        "print(\"=========================\")\n",
        "\n",
        "# Logistic Regression without PCA\n",
        "log_reg = OneVsRestLogistic(lr=0.01, n_iters=2000)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "y_pred_lr = log_reg.predict(X_test_scaled)\n",
        "acc_lr = accuracy(y_test, y_pred_lr)\n",
        "print(\"Logistic Regression accuracy:\", acc_lr)\n",
        "\n",
        "# SVM without PCA\n",
        "svm = OneVsRestSVM(lr=0.001, lambda_param=0.01, n_iters=1000)\n",
        "svm.fit(X_train_scaled, y_train)\n",
        "y_pred_svm = svm.predict(X_test_scaled)\n",
        "acc_svm = accuracy(y_test, y_pred_svm)\n",
        "print(\"Linear SVM accuracy:\", acc_svm)\n",
        "\n",
        "print(\"\\n==========================\")\n",
        "print(\"WITH PCA\")\n",
        "print(\"==========================\")\n",
        "\n",
        "# Logistic Regression with PCA\n",
        "log_reg_pca = OneVsRestLogistic(lr=0.01, n_iters=2000)\n",
        "log_reg_pca.fit(X_train_pca, y_train)\n",
        "y_pred_lr_pca = log_reg_pca.predict(X_test_pca)\n",
        "acc_lr_pca = accuracy(y_test, y_pred_lr_pca)\n",
        "print(\"Logistic Regression (PCA) accuracy:\", acc_lr_pca)\n",
        "\n",
        "# SVM with PCA\n",
        "svm_pca = OneVsRestSVM(lr=0.001, lambda_param=0.01, n_iters=1000)\n",
        "svm_pca.fit(X_train_pca, y_train)\n",
        "y_pred_svm_pca = svm_pca.predict(X_test_pca)\n",
        "acc_svm_pca = accuracy(y_test, y_pred_svm_pca)\n",
        "print(\"Linear SVM (PCA) accuracy:\", acc_svm_pca)\n",
        "\n",
        "print(\"\\n=========== SUMMARY ==========\")\n",
        "print(f\"Logistic Regression  (no PCA): {acc_lr:.4f}\")\n",
        "print(f\"Logistic Regression  (PCA)   : {acc_lr_pca:.4f}\")\n",
        "print(f\"SVM (no PCA)                 : {acc_svm:.4f}\")\n",
        "print(f\"SVM (PCA)                    : {acc_svm_pca:.4f}\")\n",
        "print(\"Original classes mapping:\", dict(enumerate(classes)))\n",
        "print(\"===============================\")\n"
      ]
    }
  ]
}